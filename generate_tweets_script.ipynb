{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a652e-8a05-4d67-8f78-d5a5bb2f9832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francomaria.nardini/guidorocchietti/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "# === Import Libraries ===\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbb8c5-a6ac-442f-a263-c078c00483f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# === Configuration ===\n",
    "\n",
    "# Set HuggingFace Transformers cache directory\n",
    "#os.environ['TRANSFORMERS_CACHE'] = '/home/francomaria.nardini/raid/guidorocchietti/.cache/huggingface'\n",
    "\n",
    "# Specify which GPUs to use\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b93f4-891b-4e8b-b49c-f173d18d9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# === Helper Functions ===\n",
    "\n",
    "def extract_first_occurrence(text):\n",
    "    \"\"\"\n",
    "    Extracts the first quoted string in the given text.\n",
    "    If no quotes are found, returns the text as is.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    match = re.search(r'\"(.*?)\"', text)\n",
    "    return match.group(1) if match else text\n",
    "\n",
    "def clean_line(text, phrase='Assistant:'):\n",
    "    \"\"\"\n",
    "    Cleans the model output by:\n",
    "    - Removing everything before the given phrase (default: 'Assistant:')\n",
    "    - Extracting the first quoted string\n",
    "    - Removing common unwanted tokens\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    start = text.find(phrase)\n",
    "    if start != -1:\n",
    "        text = text[start + len(phrase):].lstrip()\n",
    "    text = extract_first_occurrence(text)\n",
    "    text = re.sub(r'assistant\\n\\n', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe24501-58bc-4b38-91a1-3d5d21b6e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "def generate_tweet_prompt(row, sentiment='positive', prompt=None):\n",
    "    \"\"\"\n",
    "    Generates a prompt for tweet generation using metadata from a row.\n",
    "    Randomizes user demographic and social media platform.\n",
    "    \"\"\"\n",
    "    place = row['name'] if row['name'] else None\n",
    "    category = row['category']\n",
    "    \n",
    "    # Simulated user metadata\n",
    "    metadata = {\n",
    "        'place': place,\n",
    "        'category': category,\n",
    "        'sentiment': sentiment,\n",
    "        'gender': random.choice(['male', 'female', 'other']),\n",
    "        'age': random.choice(['18-24', '25-34', '35-44', '45-54', '55-64', '65+']),\n",
    "        'ethnicity': random.choice(['white', 'black', 'hispanic', 'asian', 'other']),\n",
    "        'social': random.choice(['Twitter', 'Instagram', 'Facebook', 'Tripadvisor'])\n",
    "    }\n",
    "\n",
    "    # Prompt template\n",
    "    prompt = (\n",
    "        f\"You are a creative social media post generator. \"\n",
    "        f\"Your task is to write a short, engaging, and realistic social media post based on the user's stop at Point of Interest (POI). \"\n",
    "        f\"Include the most important details, especially the **Location** and **Category** of the POI. \"\n",
    "        f\"Reflect the user's **sentiment** in the tone and style of the post. \"\n",
    "        f\"\\nHere is the information about the visit:\\n\"\n",
    "        f\"- Location: {place or 'N/A'}\\n\"\n",
    "        f\"- Category: {category or 'N/A'}\\n\"\n",
    "        f\"- Sentiment: {sentiment}\\n\"\n",
    "        f\"- Gender: {metadata['gender']}\\n\"\n",
    "        f\"- Age: {metadata['age']}\\n\"\n",
    "        f\"- Ethnicity: {metadata['ethnicity']}\\n\"\n",
    "        f\"- Social Media: {metadata['social']}\\n\"\n",
    "        f\"\\nKeep the post natural, expressive, and in the style of a post. Avoid repeating the input literallyâ€”paraphrase and add personality. \"\n",
    "        f\"Use hashtags when appropriate, especially for the location and category.\\n\"\n",
    "        f'Try not start with \"I just visited\" or \"I am at\" or \"Just spent\".\\n'\n",
    "        f\"\\nNow, based on the given information, generate a new social media post.\\n\"\n",
    "        f\"Generate only the post without any additional text. \\n\\n\"\n",
    "        f\"Post: \\n\"\n",
    "    )\n",
    "\n",
    "    return prompt, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db777aff-cf4e-4e61-8457-4eee19318cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "def generate_tweets(model, tokenizer, input_ids, attention_mask, batch_size=4, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generate tweets using the model for each input prompt.\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    model.eval()\n",
    "    for i in tqdm(range(0, len(input_ids), batch_size)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids[i:i+batch_size].to('cuda'),\n",
    "                attention_mask=attention_mask[i:i+batch_size].to('cuda'),\n",
    "                max_new_tokens=64,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                no_repeat_ngram_size=2,\n",
    "                early_stopping=True,\n",
    "                temperature=0.9,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        for output in outputs:\n",
    "            tweet = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            tweets.append(tweet)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79783238-a7f5-448d-a15f-0a36792e1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "def generate_posts(df, model, tokenizer):\n",
    "    \"\"\"\n",
    "    For each row in the dataset, generate both a positive and negative tweet,\n",
    "    tokenize the prompts, and process them with the model.\n",
    "    Returns a DataFrame of cleaned results.\n",
    "    \"\"\"\n",
    "    # Storage\n",
    "    tweets = []\n",
    "    positive_prompts, negative_prompts = [], []\n",
    "    positive_metadata, negative_metadata = [], []\n",
    "    positive_inputs, negative_inputs = [], []\n",
    "\n",
    "    # Generate prompts and tokenize\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        pos_prompt, pos_meta = generate_tweet_prompt(row, sentiment='positive')\n",
    "        neg_prompt, neg_meta = generate_tweet_prompt(row, sentiment='negative')\n",
    "\n",
    "        positive_prompts.append(pos_prompt)\n",
    "        negative_prompts.append(neg_prompt)\n",
    "        positive_metadata.append(pos_meta)\n",
    "        negative_metadata.append(neg_meta)\n",
    "\n",
    "        pos_input = tokenizer(pos_prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length')\n",
    "        neg_input = tokenizer(neg_prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length')\n",
    "\n",
    "        positive_inputs.append(pos_input)\n",
    "        negative_inputs.append(neg_input)\n",
    "\n",
    "    # Stack input tensors\n",
    "    pos_input_ids = torch.stack([x['input_ids'] for x in positive_inputs]).squeeze(1).to('cuda')\n",
    "    pos_attention = torch.stack([x['attention_mask'] for x in positive_inputs]).squeeze(1).to('cuda')\n",
    "    neg_input_ids = torch.stack([x['input_ids'] for x in negative_inputs]).squeeze(1).to('cuda')\n",
    "    neg_attention = torch.stack([x['attention_mask'] for x in negative_inputs]).squeeze(1).to('cuda')\n",
    "\n",
    "    # Generate tweets\n",
    "    pos_tweets = generate_tweets(model, tokenizer, pos_input_ids, pos_attention, batch_size=16)\n",
    "    neg_tweets = generate_tweets(model, tokenizer, neg_input_ids, neg_attention, batch_size=16)\n",
    "\n",
    "    # Clean outputs\n",
    "    def clean(texts):\n",
    "        return [\n",
    "            x.split('Post: \\n')[-1]\n",
    "             .replace('assistant\\n\\n','')\n",
    "             .replace('Post\\n','')\n",
    "             .replace('Post \\n','')\n",
    "             .replace('Post:','')\n",
    "            for x in texts\n",
    "        ]\n",
    "\n",
    "    cleaned_positive = clean(pos_tweets)\n",
    "    cleaned_negative = clean(neg_tweets)\n",
    "\n",
    "    # Create output DataFrame\n",
    "    tweets_df = pd.DataFrame({\n",
    "        'positive': cleaned_positive,\n",
    "        'positive_metadata': positive_metadata,\n",
    "        'positive_prompt': positive_prompts,\n",
    "        'negative': cleaned_negative,\n",
    "        'negative_metadata': negative_metadata,\n",
    "        'negative_prompt': negative_prompts\n",
    "    })\n",
    "\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50273212-2ab0-45d3-92be-bb6fe672ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# === Argument Parsing ===\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse CLI arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Generate tweets using a pre-trained model.\")\n",
    "    parser.add_argument('--file_path', type=str, required=True, help='Path to the input CSV file.')\n",
    "    parser.add_argument('--output_path', type=str, required=True, help='Path to save the output CSV file.')\n",
    "    parser.add_argument('--model_id', type=str, required=True, help='Model ID for the pre-trained model.')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e95017-2f74-42bc-a95f-0ee21056d714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --file_path FILE_PATH --output_path\n",
      "                             OUTPUT_PATH --model_id MODEL_ID\n",
      "ipykernel_launcher.py: error: the following arguments are required: --output_path, --model_id\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francomaria.nardini/guidorocchietti/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "# === Main Program ===\n",
    "\n",
    "\n",
    "file_path = 'PATH TO INPUT CSV OR PARQUET'\n",
    "output_path = 'PATH WHERE TO SAVE OUTPUT CSV'\n",
    "model_id = 'MODEL ID'\n",
    "\n",
    "# Load input data\n",
    "if file_path.endswith('.csv'):\n",
    "    df = pd.read_csv(file_path)\n",
    "elif file_path.endswith('.parquet'):\n",
    "    df = pd.read_parquet(file_path)\n",
    "elif file_path.endswith('.json'):\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "elif file_path.endswith('.xlsx'):\n",
    "    df = pd.read_excel(file_path)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported file format. Please provide a CSV, Parquet, JSON, or Excel file.\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Generate posts\n",
    "tweets_df = generate_posts(df, model, tokenizer)\n",
    "\n",
    "# Save to CSV\n",
    "### check extension of the file\n",
    "if output_path.endswith('.csv'):\n",
    "    tweets_df.to_csv(output_path, index=False)\n",
    "elif output_path.endswith('.parquet'):\n",
    "    tweets_df.to_parquet(output_path, index=False)\n",
    "elif output_path.endswith('.json'):\n",
    "    tweets_df.to_json(output_path, orient='records', lines=True)\n",
    "elif output_path.endswith('.xlsx'):\n",
    "    tweets_df.to_excel(output_path, index=False)\n",
    "    \n",
    "print(f\"Tweets saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
