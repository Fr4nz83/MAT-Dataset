{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14942a5c",
   "metadata": {},
   "source": [
    "# GPX to Geopandas translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9af7d-3cb3-433f-a32a-dfc0b8ce7a14",
   "metadata": {},
   "source": [
    "#### Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559a20a-fe07-4cc0-94e2-8f3044e42841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_GPX_url_tags(fname, fname_corrected) :\n",
    "    # Read the GPX file as text\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Replace <url> with <src> and </url> with </src>\n",
    "    text = text.replace(\"<url>\", \"<link1_href>\").replace(\"</url>\", \"</link1_href>\")\n",
    "    \n",
    "    # Write the modified text to a new file (or overwrite the original)\n",
    "    with open(fname_corrected, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de738dfb-9914-4a37-b468-7ca9e378a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subdirectories(directory):\n",
    "    \n",
    "    # List all entries in the given directory\n",
    "    entries = os.listdir(directory)\n",
    "    \n",
    "    # Filter out entries that are directories\n",
    "    return [os.path.join(directory, entry) for entry in entries if os.path.isdir(os.path.join(directory, entry))]\n",
    "\n",
    "\n",
    "def list_files(directory):\n",
    "    \n",
    "    # List all entries in the given directory\n",
    "    entries = os.listdir(directory)\n",
    "    \n",
    "    # Filter out entries that are files\n",
    "    return [os.path.join(directory, entry) for entry in entries if os.path.isfile(os.path.join(directory, entry))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b303bc-2ff7-4ce2-8b90-bc848e3c9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpx_to_gdf(fname_gpx : str) -> gpd.GeoDataFrame :\n",
    "\n",
    "    # 'correct_GPX_tag' replaces the <url> tags, which do not belong to the GPX standard but contain the trajectory IDs, \n",
    "    # with <link1_href>, so that GeoPandas can pick up that information.\n",
    "    fname_temporary_corrected = './corrected_file_XXX.gpx'\n",
    "    correct_GPX_url_tags(fname_gpx, fname_temporary_corrected)\n",
    "    \n",
    "    # Read the metadata associated with the trajectories in the currently considered GPX.\n",
    "    # NOTE: the index's values correspond to the 'track_fid' values in the main GeoPandas dataframe below, and will be used\n",
    "    #       to merge the metadata.\n",
    "    # NOTE 2: we use \"on_invalid='ignore'\" to skip trajectories with less than 2 points, otherwise they'd raise an exception.\n",
    "    meta_gdf = gpd.read_file(fname_temporary_corrected, layer = 'tracks', on_invalid='ignore')\n",
    "    # display(meta_gdf)\n",
    "    # display(meta_gdf.info())\n",
    "    \n",
    "    \n",
    "    # Now read the actual trajectories from the GPX.\n",
    "    list_gdf = []\n",
    "    # Read the spatio-temporal information of the trajectories from the current block.\n",
    "    gdf = gpd.read_file(fname_temporary_corrected, layer = 'track_points', on_invalid='ignore')\n",
    "    # display(gdf)\n",
    "    # display(gdf.info())\n",
    "\n",
    "    assert meta_gdf.shape[0] == gdf['track_fid'].nunique(), \"Error, different number of trajectories detected between GPX metadata and actual data!\"\n",
    "    \n",
    "    # Select the columns of interest (trajectory identifier within a GPX, timestamp, coordinates).\n",
    "    # print(\"Filtering useless columns...\")\n",
    "    selection = gdf.loc[:, ['track_fid', 'time', \"geometry\"]]\n",
    "    \n",
    "    # Use a compacter representation for 64-bit integer columns.\n",
    "    selection['track_fid'] = selection['track_fid'].astype(np.int32)\n",
    "    \n",
    "    # Drop the rows with missing or nonsensical timestamps.\n",
    "    # print(\"Filtering rows with missing or wrong timestamps...\")\n",
    "    selection.dropna(subset=['time'], inplace = True)\n",
    "    selection = selection.loc[(selection['time'] > '1990-01-01') & (selection['time'] <= str(date.today()))]\n",
    "    selection['time'] = pd.to_datetime(selection['time'])\n",
    "    # selection.info()\n",
    "    \n",
    "    # Associate a true unique identifier with trajectories.\n",
    "    # 'track_fid' represents the identifier of a trajectory within a GPX file. If, however, a trajectory is split across multiple GPXs,\n",
    "    # we cannot use it to reconstruct the trajectory. To solve the problem, we use the information from the 'link' element in a GPX file: \n",
    "    # this is the combination of a user ID AND a ID that OSM associates with a trace.\n",
    "    # This information is available from meta_gdf, so we perform a merge to put it into selection.\n",
    "    # print('Merging meta information with the trajectories...')\n",
    "    selection = selection.merge(meta_gdf['link1_href'], left_on = 'track_fid', right_index = True)\n",
    "    selection.rename(columns={'link1_href':'track_uid'}, inplace = True)\n",
    "\n",
    "    # Drop the track_fid column, which was required to merge the metadata from 'meta_gdf'\n",
    "    selection.drop(columns='track_fid', inplace = True)\n",
    "\n",
    "    # Drop the rows that have NaN values in the 'track_uid' column -- there are a few trajectories that do not have a 'username + id_traj'.\n",
    "    selection.dropna(subset=['track_uid'], inplace = True)\n",
    "    \n",
    "    # Finally, turn the 'track_uid' column into categorical, thereby compressing the trajectory identifiers.\n",
    "    selection['track_uid'] =  selection['track_uid'].astype('category')\n",
    "    # selection.info()\n",
    "\n",
    "    \n",
    "    # Remove the temporary GPX file.\n",
    "    os.remove(fname_temporary_corrected)\n",
    "    \n",
    "    # Append this dataframe to a list.    \n",
    "    return selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2a7f2-330d-4104-81ac-bf0048002239",
   "metadata": {},
   "source": [
    "#### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf821d4-f26b-4951-a0b9-97f39a8d3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_path = './gpx_traces/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d8914-35b1-49e1-8dbe-e8ab9f849ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bbox_dirs = list_subdirectories(bbox_path)\n",
    "for bbox_dir in list_bbox_dirs :\n",
    "\n",
    "    # Path to the parquet file that will store the trajectories associated with this bbox.\n",
    "    outfile_path = bbox_path + bbox_dir.split('/')[-1] + '.parquet'\n",
    "\n",
    "    # Check if this bbox's parquet has already been generated. If so, skip to the next bbox.\n",
    "    if os.path.isfile(outfile_path) : \n",
    "        print(f'{outfile_path} already exist, hence skipping.')\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # Process the GPXs within bbox_dir...\n",
    "    list_gdfs = []\n",
    "    list_bbox_files = list_files(bbox_dir)\n",
    "    for fname in tqdm(list_bbox_files, desc=f\"Processing files in {bbox_dir}\"):\n",
    "        # print(f'Processing {fname}...')\n",
    "        list_gdfs.append(gpx_to_gdf(fname))\n",
    "\n",
    "    \n",
    "    # Concatenate the geodataframes related to the current bbox...\n",
    "    print(f'Preparing the final concatenated dataframe...')\n",
    "    combined = gpd.GeoDataFrame(pd.concat(list_gdfs, ignore_index=True), crs=list_gdfs[0].crs)\n",
    "    # Ensure that the 'track_uid' is categorical.\n",
    "    combined['track_uid'] = combined['track_uid'].astype('category')\n",
    "    # Finally, remove the small fraction of duplicate rows (i.e., those having same timestamp, location, and ID) \n",
    "    # that might be present in the final Dataframe.\n",
    "    combined.drop_duplicates(ignore_index = True, inplace = True)\n",
    "    # combined.info()\n",
    "    \n",
    "    # Write the GeoDataFrame into a parquet.\n",
    "    print(f'Writing GeoDataFrame to {outfile_path}...')\n",
    "    combined.to_parquet(outfile_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
