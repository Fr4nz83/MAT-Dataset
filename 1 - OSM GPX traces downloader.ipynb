{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf05598-d170-46c0-9a22-969a15d50b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d36d6-3bdd-42f6-9dbc-2b6f93212665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_files_bboxdir(bbox_path):\n",
    "    if os.path.isdir(bbox_path):\n",
    "        return [file for file in os.listdir(bbox_path) if os.path.isfile(os.path.join(bbox_path, file))]\n",
    "    return None\n",
    "\n",
    "def extract_number_from_string(s : str) -> int:\n",
    "    return int(re.findall(r'\\d+', s)[0])\n",
    "\n",
    "def first_discontinuity_index(list_pages):\n",
    "    # If a list is empty, return 0.\n",
    "    if len(list_pages) == 0 : return 0\n",
    "        \n",
    "    list_pages = sorted(list_pages)  # Ensure the list is sorted\n",
    "    for i in range(len(list_pages) - 1):\n",
    "        if list_pages[i] + 1 != list_pages[i + 1]:\n",
    "            return i + 1  # Return the index of the missing page.\n",
    "    \n",
    "    return -1  # Return -1 if no discontinuities are found\n",
    "\n",
    "\n",
    "# Example usage\n",
    "#directory = \"./gpx_traces/bbox_3\"\n",
    "#files = check_files_bboxdir(directory)\n",
    "#if files is not None:\n",
    "#    pages = {extract_number_from_string(s) for s in files}\n",
    "#    # print(pages)\n",
    "#    print(first_discontinuity_index(pages))\n",
    "#else:\n",
    "#    print(\"Directory does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a36ea7-d8b1-4482-a8a6-0ed391e71406",
   "metadata": {},
   "source": [
    "### Parallel batch-continuous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750ae46-ff6b-4ea4-be19-6cdf3e7e72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bounding_boxes(bbox : tuple[float,float,float,float], step : float) -> list[tuple[float,float,float,float]]:\n",
    "\n",
    "    list_bboxes = []\n",
    "    \n",
    "    min_lon = bbox[0]\n",
    "    min_lat = bbox[1]\n",
    "    max_lon = bbox[2]\n",
    "    max_lat = bbox[3]\n",
    "    for curr_lon in np.arange(min_lon, max_lon, step) :\n",
    "        for curr_lat in np.arange(min_lat, max_lat, step) :\n",
    "            list_bboxes.append(f'{curr_lon},{curr_lat},{min(curr_lon + step, max_lon)},{min(curr_lat + step, max_lat)}')\n",
    "\n",
    "    return list_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e85afb-9c53-423a-991b-2a79686282c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(bbox, page, sleeptime):    \n",
    "    url = f'{api_url}?bbox={bbox}&page={page}'\n",
    "    print(f'Sending HTTP request to {url}...')\n",
    "    # headers = {'User-Agent': 'JupyterNotebook/1.0'}\n",
    "    headers = {'User-Agent': 'Notebook/1.0'}\n",
    "\n",
    "    while True :\n",
    "        try :\n",
    "            time.sleep(sleeptime)\n",
    "            response = requests.get(url, headers=headers, timeout = 120)\n",
    "            return page, response\n",
    "\n",
    "        # If an exception occurs, e.g., malformed response, we resubmit the request after a small pause.\n",
    "        except Exception as err:\n",
    "            print(f'Exception {err} occurred for request {url}, trying again...')\n",
    "            time.sleep(sleeptime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862dbd22-5459-449b-867a-a9f78ed0dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the overall bbox in smaller bboxes such that the API request will be accepted by OSM.\n",
    "# NOTE: limit_deg should be 0.25, but beyond 0.06 the OSM API don't seem to work properly. Probably there's another limit in place too.\n",
    "bbox_NY = (-74.259,40.477,-73.700,40.918)\n",
    "limit_deg = 0.06\n",
    "list_bboxes = compute_bounding_boxes(bbox_NY, limit_deg)\n",
    "print(f'Number of bounding boxes to download: {len(list_bboxes)}')\n",
    "\n",
    "\n",
    "# OSM API endpoint for GPS trackpoints\n",
    "# NOTE: when using the OSM API, we have to use max 2 download threads, otherwise the IP will get banned.\n",
    "#       See also https://operations.osmfoundation.org/policies/api/\n",
    "api_url = 'https://api.openstreetmap.org/api/0.6/trackpoints'\n",
    "\n",
    "\n",
    "# Directory to save the downloaded GPX files\n",
    "output_dir = 'gpx_traces'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "max_workers = 2 # Limit download threads for OSM API is 2. More than that and the IP will get banned.\n",
    "idx_bbox = 0\n",
    "list_bboxes = list_bboxes[idx_bbox :]\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    for bbox in list_bboxes :\n",
    "        print(f'Processing {idx_bbox}-th bbox: {bbox}')\n",
    "\n",
    "        # Check if the directory for this bbox already exists and, if this is true, which pages have already been downloaded.\n",
    "        files = check_files_bboxdir(f'{output_dir}/bbox_{str(idx_bbox)}')\n",
    "        next_page = 0\n",
    "        if files is not None: \n",
    "            pages = {extract_number_from_string(s) for s in files}\n",
    "            idx_check = first_discontinuity_index(pages)\n",
    "            \n",
    "            print(f'Bbox dir detected, discontinuity check: {idx_check}')\n",
    "            if idx_check == -1 :\n",
    "                print('We have already downloaded all the data for this bbox, go to the next one')\n",
    "                idx_bbox += 1\n",
    "                continue # No discontinuities detected: assume that this bbox's data has already been completely downloaded. \n",
    "            else : next_page = idx_check # Discontinuity detected: restart downloading the bbox's data from idx_check.\n",
    "        \n",
    "        else:\n",
    "            print(f'Bbox dir not detected, download everything.')\n",
    "            os.makedirs(f'{output_dir}/bbox_{str(idx_bbox)}', exist_ok=False)\n",
    "\n",
    "        \n",
    "        ### Start downloading the pages. ###\n",
    "        # Submit an initial batch of tasks equal to max_workers\n",
    "        future_to_page = {executor.submit(fetch_page, bbox, p, 2.0): p for p in range(next_page, next_page + max_workers)}\n",
    "        next_page += max_workers\n",
    "        while future_to_page:\n",
    "            # Iterate over completed futures\n",
    "            for future in concurrent.futures.as_completed(list(future_to_page.keys())):\n",
    "                \n",
    "                p, response = future.result()\n",
    "                \n",
    "                # Case 1 - we have succesfully downloaded a page.\n",
    "                if response.status_code == 200 and '<trkpt' in response.text:\n",
    "                    gpx_filename = os.path.join(output_dir, 'bbox_' + str(idx_bbox), f'trackpoints_page_{p}.gpx')\n",
    "                    with open(gpx_filename, 'w', encoding='utf-8') as file:\n",
    "                        file.write(response.text)\n",
    "                    print(f'Saved: {gpx_filename}')\n",
    "                    \n",
    "                    # Submit a new task to keep the pool full.\n",
    "                    future_to_page[executor.submit(fetch_page, bbox, next_page, 2.0)] = next_page\n",
    "                    next_page += 1\n",
    "\n",
    "                \n",
    "                # Case 2 - timeout or a page does not exist.\n",
    "                else:\n",
    "                    # Case 2.1 - Page does not exist because the bbox's data has been exausted. Let the worker finish its execution.\n",
    "                    if response.status_code == 200 : \n",
    "                        print(f'No trace data available for page {p}, response code: {response.status_code}.')\n",
    "                        \n",
    "                    # Case 2.2 - timeout because bbox is too big or the server is under heavy load. In this case: retry!\n",
    "                    #            NOTE: If the bbox is too big then reduce its size, or the server might always go in timeout.\n",
    "                    else :\n",
    "                        print(f'Timeout for page {p}, response code: {response.status_code}, retry.')\n",
    "                        # print(f'response code: {response.text}')\n",
    "                        # Resubmit the task that went in timeout.\n",
    "                        future_to_page[executor.submit(fetch_page, bbox, p, 5.0)] = p\n",
    "\n",
    "                \n",
    "                # Remove the completed future from the dictionary.\n",
    "                del future_to_page[future]\n",
    "\n",
    "        # Update the index of the bounding box that'll be considered next.\n",
    "        idx_bbox += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
