{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14942a5c",
   "metadata": {},
   "source": [
    "# Single GPX file to Geopandas translation\n",
    "\n",
    "In this notebook, we translate the trajectories and associated metadata contained in a single GPX file, such as those downloaded with JOSM, to a Geopandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9af7d-3cb3-433f-a32a-dfc0b8ce7a14",
   "metadata": {},
   "source": [
    "#### Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9090e1-3533-48e4-9ef3-9f56e7605bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata_gpx(fname_gpx : str) -> gpd.GeoDataFrame :\n",
    "    '''\n",
    "    Read the metadata associated with the trajectories in the 'fname_gpx' GPX file.\n",
    "    \n",
    "    NOTE: the index's values correspond to the 'track_fid' values in the main GeoPandas dataframe below, and will be used\n",
    "          to merge the metadata.\n",
    "    NOTE 2: we use \"on_invalid='ignore'\" to skip trajectories with less than 2 points, otherwise they'd raise an exception.\n",
    "    '''\n",
    "    \n",
    "    meta_gdf = gpd.read_file(fname_gpx, layer = 'tracks', on_invalid='ignore')\n",
    "    meta_gdf = meta_gdf.loc[:, ['name', 'desc', \"link1_href\", 'geometry']]\n",
    "    return meta_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b303bc-2ff7-4ce2-8b90-bc848e3c9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpx_to_gdf(fname_gpx : str) -> gpd.GeoDataFrame :\n",
    "    '''\n",
    "    Read the actual trajectories from the 'fname_gpx' GPX file.\n",
    "    '''\n",
    "    \n",
    "    list_gdf = []\n",
    "    step = int(2e6)\n",
    "    for i in range(0, int(1000e6), step) :\n",
    "        print(f\"Processing trajectories in the block of points [{str(i)} -- {str(i + step)})\")\n",
    "        \n",
    "        gdf = gpd.read_file(fname_gpx, layer = 'track_points', on_invalid='ignore', rows = slice(i, i + step))\n",
    "        # gdf.info()\n",
    "    \n",
    "        # Early exit: when there are no more rows to read, terminate the loop.\n",
    "        if gdf.shape[0] == 0 :\n",
    "              print(\"No more points to process. Exiting the loop...\")\n",
    "              break\n",
    "\n",
    "        # Select the columns of interest (trajectory identifier within a GPX, timestamp, coordinates).\n",
    "        # print(\"Filtering useless columns...\")\n",
    "        selection = gdf.loc[:, ['track_fid', 'time', \"geometry\"]]\n",
    "\n",
    "        # Append this dataframe to a list.    \n",
    "        list_gdf.append(selection.copy(deep=True))\n",
    "\n",
    "\n",
    "    # Concatenate the dataframes created previously.\n",
    "    final = gpd.GeoDataFrame(pd.concat(list_gdf, ignore_index=True), crs = list_gdf[0].crs)\n",
    "    final.info()\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65484c-2b01-484d-9f50-f583c10359a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(metadata_df : gpd.GeoDataFrame, trajs_df : gpd.GeoDataFrame) -> gpd.GeoDataFrame :\n",
    "    \n",
    "    # Associate a true unique identifier with trajectories.\n",
    "    # 'track_fid' represents the identifier of a trajectory within a GPX file. If, however, a trajectory is split across multiple GPXs,\n",
    "    # we cannot use it to reconstruct the trajectory. To solve the problem, we use the information from the 'link' element in a GPX file: \n",
    "    # this is the combination of a user ID AND a ID that OSM associates with a trace.\n",
    "    # This information is available from meta_gdf, so we perform a merge to put it into selection.\n",
    "    # print('Merging meta information with the trajectories...')\n",
    "    selection = trajs_df.merge(metadata_df['link1_href'], left_on = 'track_fid', right_index = True)\n",
    "    selection.rename(columns={'link1_href':'uid'}, inplace = True)\n",
    "    # display(selection)\n",
    "    display(selection.info())\n",
    "    \n",
    "    # Turn the 'uid' column into categorical, thereby compressing the trajectory identifiers.\n",
    "    # Drop also the track_fid column, which was required to merge the metadata.\n",
    "    selection['uid'] =  selection['uid'].astype('category')\n",
    "    selection.drop(columns='track_fid', inplace = True)\n",
    "    display(selection.info())\n",
    "    \n",
    "    # Drop the rows of trajectories for which it is not possible to understand the user behind them.\n",
    "    selection.dropna(subset=['uid'], inplace = True)\n",
    "    display(selection.info())\n",
    "    \n",
    "    # Drop the rows with missing or nonsensical timestamps.\n",
    "    selection.dropna(subset=['time'], inplace = True)\n",
    "    # selection = selection.loc[(selection['time'] > '1990-01-01') & (selection['time'] <= str(date.today()))]\n",
    "    selection['time'] = pd.to_datetime(selection['time'])\n",
    "    display(selection.info())\n",
    "    \n",
    "    # Remove duplicate rows, i.e., those having same timestamp, geometry, and uid.\n",
    "    # GPX files downloaded from JOSM have LOTS of duplicated trajectories, so we need to take care of them.\n",
    "    selection.drop_duplicates(ignore_index = True, inplace = True)\n",
    "    display(selection.info())\n",
    "\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9efb609-f479-4c89-bbca-8260809b3975",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e808e-a5f6-4671-8986-0911cb19e32d",
   "metadata": {},
   "source": [
    "#### Read the metadata and the trajectory information within a GPX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487e2ee-baaa-4791-8020-b9a05d7d23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a few filenames.\n",
    "gpx_path = './Traiettorie Parigi'\n",
    "gpx_name = 'paris_centre'\n",
    "gpx_filename = os.path.join(gpx_path, gpx_name + '.gpx')\n",
    "meta_geodf_filename = os.path.join(gpx_path, gpx_name + '.meta.parquet')\n",
    "geodf_filename = os.path.join(gpx_path, gpx_name + '.parquet')\n",
    "trajdf_filename = os.path.join(gpx_path, gpx_name + '.processed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95723b02-32ab-4a0f-88c6-78f2ba2e0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata from a GPX file, or read an existing parquet with such info.\n",
    "if not os.path.isfile(meta_geodf_filename) :\n",
    "    print(f'Parsing metadata from GPX...')\n",
    "    meta_geo_df = read_metadata_gpx(gpx_filename)\n",
    "    meta_geo_df.to_parquet(meta_geodf_filename)\n",
    "else :\n",
    "    print(f'Reading metadata from an existing parquet file...')\n",
    "    meta_geo_df = gpd.read_parquet(meta_geodf_filename)\n",
    "\n",
    "print(meta_geo_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ec48e-86b2-42b5-9e2a-649e41af12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a single big GPX to a Geopandas Dataframe.\n",
    "if not os.path.isfile(geodf_filename) :\n",
    "    print(f'Parsing trajectories from GPX...')\n",
    "    geo_df = gpx_to_gdf(gpx_filename)\n",
    "    geo_df.to_parquet(geodf_filename)\n",
    "else :\n",
    "    print(f'Reading trajectories from an existing parquet file...')\n",
    "    geo_df = gpd.read_parquet(geodf_filename)\n",
    "\n",
    "print(geo_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c9705-70a4-450b-8c8d-49dc51ab0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(meta_geo_df)\n",
    "display(geo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43edab69-889a-4ecf-b955-eb3307efd335",
   "metadata": {},
   "source": [
    "#### Preprocess the metadata and the trajectories\n",
    "\n",
    "Associate the metadata with the trajectories, drop useless info, drop trajectories without a user,  drop trajectories with impossible timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0bc5d6-544d-453b-8172-709c68916850",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = process_dataset(meta_geo_df, geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d583ee7-3d7b-4912-962b-30ca34252364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the GeoDataFrame into a parquet.\n",
    "print(f'Writing GeoDataFrame to {trajdf_filename}...')\n",
    "final_dataset.to_parquet(trajdf_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
